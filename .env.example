# This file provides an example of the environment variables needed for different LLM providers.
# Copy this file to .env and fill in the values for the provider you want to use.
# Remember to add .env to your .gitignore file!

# --- OpenAI ---
# Used for models like "openai/gpt-4o"
OPENAI_API_KEY="your-openai-api-key"

# --- Google ---
# Used for models like "gemini/gemini-2.0-flash"
GEMINI_API_KEY="your-gemini-api-key"

# --- Azure OpenAI ---
# Used for models like "azure/your-deployment-name"
AZURE_API_KEY="your-azure-api-key"
AZURE_API_BASE="https://your-resource-name.openai.azure.com/"
AZURE_API_VERSION="2024-02-01"

# --- Ollama ---
# No API key needed if running Ollama locally. 
# LiteLLM will connect to http://localhost:11434 by default.
# For a remote Ollama server, you can set the base URL:
# OLLAMA_API_BASE="http://your-ollama-host:11434"
